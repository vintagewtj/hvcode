import torch
from torch import nn
import config
import os
from main_parser import *


# os.environ['CUDA_VISIBLE_DEVICES'] = '0'
device = torch.device("cuda:0")
# device_ids = list(range(torch.cuda.device_count()))

# class Sino_Conv1(nn.Module):
#     def __init__(self, in_chs, out_chs):
#         super(Sino_Conv1, self).__init__()
#         self.sino_conv1 = nn.Sequential(
#             nn.Conv2d(in_chs, out_chs, 3, padding='same'),
#             nn.ReLU(inplace=True),
#         )
#     def forward(self, x):
#         x = self.sino_conv1(x)
#         return x
#
#
# class Sino_Conv2(nn.Module):
#     def __init__(self, in_chs, out_chs):
#         super(Sino_Conv2, self).__init__()
#         self.sino_conv2 = nn.Sequential(
#             nn.Conv2d(in_chs, out_chs, 3, padding='same'),
#             nn.BatchNorm2d(out_chs),
#             nn.ReLU(inplace=True),
#             nn.Conv2d(in_chs, out_chs, 3, padding='same'),
#             nn.BatchNorm2d(out_chs),
#             nn.ReLU(inplace=True),
#             nn.Conv2d(in_chs, out_chs, 3, padding='same'),
#             nn.BatchNorm2d(out_chs),
#             nn.ReLU(inplace=True),
#             nn.Conv2d(in_chs, out_chs, 3, padding='same'),
#             nn.BatchNorm2d(out_chs),
#             nn.ReLU(inplace=True),
#         )
#     def forward(self, x):
#         x = self.sino_conv2(x)
#         return x
#
# class Sino_Conv3(nn.Module):
#     def __init__(self, in_chs, out_chs):
#         super(Sino_Conv3, self).__init__()
#         self.sino_conv3 = nn.Conv2d(in_chs, out_chs, 3, padding='same')
#     def forward(self, x):
#         x = self.sino_conv3(x)
#         return x

class Maxpool(nn.Module):
    def __init__(self):
        super(Maxpool, self).__init__()
        self.max_pool = nn.MaxPool2d(kernel_size=2, padding=0)
    def forward(self,x):
        x = self.max_pool(x)
        return x

class CT_Conv1(nn.Module):
    def __init__(self, in_chs, out_chs):
        super(CT_Conv1, self).__init__()
        self.ct_conv1 = nn.Sequential(
            nn.Conv2d(in_chs, out_chs, 3, padding='same'),
            nn.BatchNorm2d(out_chs),
            nn.ReLU(inplace=True)
        )
    def forward(self, x):
        x = self.ct_conv1(x)
        return x

class CT_Conv2(nn.Module):
    def __init__(self, in_chs, out_chs):
        super(CT_Conv2, self).__init__()
        self.ct_conv2 = nn.Sequential(
            nn.Conv2d(in_chs, out_chs, 1, padding='same'),
        )
    def forward(self, x):
        x = self.ct_conv2(x)
        return x

class CT_Deconv1(nn.Module):
    def __init__(self, in_chs, out_chs):
        super(CT_Deconv1, self).__init__()
        self.ct_deconv1 = nn.Sequential(
            nn.ConvTranspose2d(in_chs, out_chs, kernel_size=3, stride=2, padding=1, output_padding=1),
            nn.BatchNorm2d(out_chs),
            nn.ReLU(inplace=True),
        )
    def forward(self, x):
        x = self.ct_deconv1(x)
        return x


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # self.conv0 = Sino_Conv1(1, 64)
        # self.conv1 = Sino_Conv2(64, 64)
        # self.conv2 = Sino_Conv3(64, 1)

        self.conv3 = CT_Conv1(1, 64)
        self.conv4 = CT_Conv1(64, 64)
        self.down0 = Maxpool()
        self.conv5 = CT_Conv1(64, 128)
        self.conv6 = CT_Conv1(128, 128)
        self.down1 = Maxpool()
        self.conv7 = CT_Conv1(128, 256)
        self.conv8 = CT_Conv1(256, 256)
        self.down2 = Maxpool()
        self.conv9 = CT_Conv1(256, 512)
        self.conv10 = CT_Conv1(512, 512)
        self.down3 = Maxpool()
        self.conv11 = CT_Conv1(512, 1024)
        self.conv12 = CT_Conv1(1024, 1024)
        self.deconv0 = CT_Deconv1(1024, 512)#
        self.conv13 = CT_Conv1(1024, 512)
        self.conv14 = CT_Conv1(512, 512)
        self.deconv1 = CT_Deconv1(512, 256)#
        self.conv15 = CT_Conv1(512, 256)
        self.conv16 = CT_Conv1(256, 256)
        self.deconv2 = CT_Deconv1(256, 128)#
        self.conv17 = CT_Conv1(256, 128)
        self.conv18 = CT_Conv1(128, 128)
        self.deconv3 = CT_Deconv1(128, 64)#
        self.conv19 = CT_Conv1(128, 64)
        self.conv20 = CT_Conv1(64, 64)
        self.conv21 = CT_Conv2(64, 1)
        self.relu = nn.PReLU()
        # self.init_weights(0)

    def forward(self, x):
        # x1 = self.conv0(x)
        # x2 = self.conv1(x1)
        # x3 = self.conv2(x2)
        # x3 = x3 + x
        # x4 = config.fbp_op_mod(x3).clamp(min=0, max=1)
        # x4 = 0 + (1 - 0) / (torch.max(x4) - torch.min(x4)) * (x4 - torch.min(x4))

        x5 = self.conv3(x)
        x6 = self.conv4(x5)#
        x7 = self.down0(x6)
        x8 = self.conv5(x7)
        x9 = self.conv6(x8)#
        x10 = self.down1(x9)
        x11 = self.conv7(x10)
        x12 = self.conv8(x11)#
        x13 = self.down2(x12)
        x14 = self.conv9(x13)
        x15 = self.conv10(x14)#
        x16 = self.down3(x15)
        x17 = self.conv11(x16)
        x18 = self.conv12(x17)
        x19 = torch.concat((x15, self.deconv0(x18)), 1)
        x20 = self.conv13(x19)
        x21 = self.conv14(x20)
        x22 = torch.concat((x12, self.deconv1(x21)), 1)
        x23 = self.conv15(x22)
        x24 = self.conv16(x23)
        x25 = torch.concat((x9, self.deconv2(x24)), 1)
        x26 = self.conv17(x25)
        x27 = self.conv18(x26)
        x28 = torch.concat((x6, self.deconv3(x27)), 1)
        x29 = self.conv19(x28)
        x30 = self.conv20(x29)
        x31 = self.conv21(x30) + x
        y = self.relu(x31)
        # y = 0 + (1 - 0) / (torch.max(y) - torch.min(y)) * (y - torch.min(y))
        return y

def weight_init(m):
    if isinstance(m, nn.Linear):
        nn.init.xavier_normal_(m.weight)
        nn.init.constant_(m.bias, 0)
    # 也可以判断是否为conv2d，使用相应的初始化方式
    elif isinstance(m, nn.Conv2d):
        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
    # 是否为批归一化层
    elif isinstance(m, nn.GroupNorm):
        nn.init.constant_(m.weight, 1)
        nn.init.constant_(m.bias, 0)

def count_para(model):
    param_count = 0
    for param in model.parameters():
        param_count += param.view(-1).size()[0]
    return param_count

if __name__ == '__main__':
    model = Net()
    model.to(device)
    with torch.no_grad():
        input = torch.ones(1, 1, 512, 512)
        input = input.to(device)
        output = model(input)
        output = output.to(device)
    print(output.shape)
    # print(output)
    param = count_para(model)
    print('Total parameters: %.2fM (%d)' % (param / 1e6, param))
